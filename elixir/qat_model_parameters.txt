Name: layer1.scale
Value: 5.8069939613342285

Name: layer1.zero_point
Value: 155

Name: layer1._packed_params.dtype
Value: torch.qint8

Name: layer1._packed_params._packed_params
Value: (tensor([[ 0.1909, -0.3817, -0.0477,  ..., -0.4294, -0.0477, -0.0477],
        [ 0.1909, -0.3817, -0.2863,  ...,  0.0000,  0.0477,  0.1909],
        [-1.9085,  0.1909, -0.3817,  ...,  0.0477, -0.0954,  0.0000],
        ...,
        [-0.2863, -0.9065, -0.2863,  ..., -1.1928, -0.7634, -0.1909],
        [ 0.0000, -0.2863, -0.0954,  ..., -0.1909, -0.2863,  0.2386],
        [-1.0497,  0.4294, -0.2863,  ...,  0.8111, -0.2386, -0.1909]],
       size=(1024, 7), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.04771285131573677,
       zero_point=27), Parameter containing:
tensor([ 0.1467, -0.2225, -1.5777,  ...,  0.0220,  0.0781, -0.6837],
       requires_grad=True))

Name: layer2.scale
Value: 224.991943359375

Name: layer2.zero_point
Value: 168

Name: layer2._packed_params.dtype
Value: torch.qint8

Name: layer2._packed_params._packed_params
Value: (tensor([[ 0.0000,  0.0000, -0.1119,  ...,  0.0000,  0.0000, -0.1119],
        [ 0.0000,  0.0000,  0.1119,  ..., -0.1119,  0.0000, -0.1119],
        [ 0.0000,  0.0000,  0.1119,  ..., -0.1119,  0.0000,  0.0000],
        ...,
        [-0.1119,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1119],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.4475,  0.0000,  0.1119],
        [ 0.0000,  0.0000, -0.1119,  ...,  0.0000,  0.0000, -0.2238]],
       size=(512, 1024), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.11187844723463058,
       zero_point=32), Parameter containing:
tensor([-0.0621, -0.0422, -0.0614, -0.0718, -0.0711, -0.1990, -1.8239, -2.2478,
        -0.0630, -0.0395, -0.1791, -0.0471, -0.0944, -0.2815, -0.1427, -0.1338,
        -2.0596, -0.0730, -0.0785, -0.0559, -2.3191, -1.8600, -0.0900, -0.0520,
        -0.0459, -0.0719, -0.2852, -2.1815, -2.4667, -1.6226, -2.6434, -2.0010,
        -2.4558, -1.7309, -0.0924, -2.3742, -2.3459, -0.0758, -2.9498, -0.0859,
        -2.3234, -0.0424, -0.0371, -2.3998, -0.0630, -0.0541, -2.4054, -2.5449,
        -0.3709, -0.2109, -2.3091, -0.2347, -0.0618, -0.2970, -0.0541, -0.0458,
        -0.2740, -0.0891, -0.0502, -1.3237, -0.0825, -2.1753, -0.0365, -2.4008,
        -0.0603, -0.0695, -2.4194, -0.0478, -2.5396, -2.3954, -0.3490, -0.0494,
        -0.0507, -0.0819, -0.0598, -0.0520, -1.6675, -2.3802, -0.0522, -2.4618,
        -0.0712, -0.0335, -0.0386, -2.0965, -2.5254, -2.3496, -0.0838, -0.0401,
        -2.5636, -0.0532, -1.8990, -1.8024, -0.0986, -0.0537, -2.2187, -0.0311,
        -2.6014, -2.4421, -2.3418, -0.0775, -0.1846, -0.0296, -0.2064, -2.4756,
        -0.0590, -1.9098, -2.0751, -0.1862, -0.1047, -2.1379, -1.4322, -0.0501,
        -2.3637, -0.1545, -0.2006, -0.0388, -0.0337, -0.0547, -2.7347, -1.8160,
        -0.0656, -0.0371, -2.6093, -0.0562, -2.3418, -2.4296, -0.0669, -2.2887,
        -0.0399, -2.3819, -0.3322, -0.0482, -0.0714, -2.3805, -2.3015, -1.9850,
        -0.1919, -0.0330, -1.9089, -0.2186, -0.0337, -2.2618, -2.6313, -2.4461,
        -0.0577, -0.0398, -0.0788, -0.0750, -2.7438, -1.9762, -1.4371, -2.5387,
        -0.2184, -2.4966, -0.0532, -3.0134, -0.0825, -2.4575, -0.0729, -2.5191,
        -2.5804, -2.4443, -0.0520,  0.3411, -0.0537, -0.0321, -2.5382, -2.5679,
        -0.0340, -1.9787, -0.0528, -2.0877, -0.4389, -0.0516, -2.5855, -0.0517,
        -0.0580, -0.0872, -0.0882, -2.3931, -1.7410, -0.0427, -0.9970, -1.2835,
        -0.1692, -0.0846, -0.0862, -2.1024, -0.0718, -1.9051, -0.0583, -0.0819,
        -0.0855, -0.0647, -0.0745, -0.0764, -0.0699, -0.2642, -2.1205, -2.5818,
        -0.0433, -2.4531, -0.1535, -0.0829, -1.5445, -2.5229, -1.9338, -0.0830,
        -0.0759, -0.0678, -0.0997, -2.5071, -2.1817, -2.0993, -0.1072, -0.0655,
        -0.0267, -2.9758, -2.5460, -0.1652, -0.0320, -0.1681, -0.0741, -0.0846,
        -0.0739, -2.6401, -2.6582, -2.1773, -0.0864, -0.0358, -2.2163, -0.0909,
        -2.3009, -0.0609, -2.4184, -0.0599, -1.0101, -0.7672, -2.9689, -1.8085,
        -2.2947, -2.2276, -1.9705, -0.0452, -0.0887, -0.4681, -0.0202, -0.0387,
        -2.4678, -2.3840,  0.1350, -0.0430, -0.0673, -2.4036, -2.4157, -2.6089,
        -1.5069,  0.0254, -0.0884, -0.0402, -1.5670, -0.0654, -0.2498, -2.4070,
        -2.3267, -1.7723, -0.1517, -2.1380, -2.6377, -0.2766, -2.0737, -2.2784,
        -2.3743, -0.0781, -2.2984, -2.4684, -0.0325, -0.0799, -0.0844, -0.0883,
        -0.0616, -0.0802, -0.0487, -2.4636, -2.1794, -0.0819, -0.0875, -0.0639,
        -2.2567, -0.0278, -2.1764, -0.4009, -1.1235, -0.0754, -2.8741, -0.0509,
        -0.0856, -0.0346, -1.8611, -0.0789, -0.0416, -2.4290, -0.0322, -2.0327,
        -0.0396, -0.0555, -2.8020, -0.2724, -0.0900, -3.7759, -1.5480, -0.0321,
        -2.2635, -0.0768,  0.9938, -0.1195, -2.6924, -0.0444, -2.2243, -0.2777,
        -0.5916, -0.1096, -2.0360, -2.1734, -0.0902, -0.0555, -2.3864, -0.0452,
        -0.1679, -1.4747, -1.7695, -0.0852, -0.0494, -2.1556, -0.0541, -0.0333,
        -0.0393, -0.0714, -0.0832, -1.7483, -1.8006, -1.9155, -0.3228, -0.0858,
        -0.0507, -2.3570, -2.0971, -0.1886, -2.1104, -2.3240, -2.1032, -0.0287,
        -0.0873, -0.0403, -0.0395, -0.0349, -2.0633, -0.0708, -1.7570, -2.4303,
        -1.6276, -0.0752, -0.1174, -0.2833, -2.3605, -0.1876,  3.9851, -0.0696,
        -0.0869, -0.1967, -0.0478, -1.9211, -2.3892, -2.4170, -0.0690, -0.0739,
        -2.1738, -0.0618, -0.0475, -1.4144, -0.0705, -0.1846, -2.4633, -2.4973,
        -0.1112, -0.0672, -0.0425, -2.0820, -2.5056, -0.0631, -3.0048, -2.2408,
        -2.3928,  0.6334, -0.0606, -0.0299, -0.0357, -0.0555, -0.3800, -1.9306,
        -0.0751, -0.0791, -0.0795, -2.1876, -0.0641, -0.0808, -0.2686, -1.8331,
        -0.0475, -1.6895, -0.0298, -0.7867, -0.1059, -0.0653, -0.0456, -0.0890,
        -0.0301, -0.0802, -2.3066, -2.4015, -0.2876, -0.0298, -0.0998, -2.5897,
        -2.3119, -2.2391, -0.0659, -2.3599, -2.1788, -0.0751, -2.3923, -1.6295,
        -2.1724, -0.0504, -1.9685, -0.0642, -0.0792, -2.4149, -0.0378, -2.3576,
        -2.1385, -0.0769, -2.5709, -0.0751, -0.0873, -0.0786,  0.0923, -0.7278,
        -0.0458, -1.4472, -0.3531, -2.2804, -0.0868, -0.0580, -0.0445, -0.2855,
        -0.0867, -0.3120, -1.9263, -0.0562, -0.0321, -0.3194, -1.8351, -0.0614,
        -1.8309, -2.1376, -1.8420, -2.7067, -0.0518, -0.0673, -2.1762, -0.0825,
        -0.0785, -2.4841, -2.4635, -0.0908, -2.2190, -0.0344, -0.0447, -2.4809,
        -0.0338, -1.5983, -0.0678, -2.8071, -0.0906, -2.4525, -2.1184, -0.0543,
        -0.0638, -0.1565, -0.0871, -0.0243, -2.3725, -0.0908, -0.1868, -0.0465,
        -3.9225, -2.2838, -0.1775, -0.0828, -2.1758, -2.5047, -0.0440, -0.0327,
         0.8552, -2.1706, -0.2972, -1.9511, -0.0361, -0.1147, -0.3472, -0.3693],
       requires_grad=True))

Name: layer3.scale
Value: 548.3836669921875

Name: layer3.zero_point
Value: 185

Name: layer3._packed_params.dtype
Value: torch.qint8

Name: layer3._packed_params._packed_params
Value: (tensor([[ 0.0458, -0.0458, -0.0458,  ...,  0.0916, -0.2290,  0.0000],
        [ 0.0000, -0.0458,  0.0000,  ..., -0.0458,  0.0458,  0.0000],
        [-0.0458,  0.0000, -0.0458,  ...,  0.0458, -0.2748,  0.0458],
        ...,
        [-0.0916, -0.0916, -0.0916,  ..., -0.0458, -0.0916, -0.0458],
        [ 0.0916,  0.0458,  0.0916,  ..., -0.0916,  0.0458,  0.0000],
        [-0.0458, -0.0458, -0.0916,  ..., -0.0458, -0.4121,  0.0458]],
       size=(128, 512), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.04579317942261696,
       zero_point=26), Parameter containing:
tensor([-3.6506, -0.0795, -2.5360, -0.3165, -0.3738, -0.3333, -4.6964, -0.3542,
        -1.4411, -2.0183, -0.7968, -0.0368, -0.0295, -0.2360,  0.5719, -1.1486,
        -0.6633, -1.7820, -0.0507, -0.5540, -1.1465, -1.1070, -1.4753,  2.0108,
         1.5045, -1.6511, -0.4563,  0.8511, -0.1677,  0.9044, -1.3937, -0.2401,
        -0.0435, 11.9632, -0.5477, -0.7010, -0.1612, -0.8123, -0.1092, -0.2231,
        -0.1010, -0.3812, -0.3521,  0.4327, -1.5245, -1.4370, -0.3309, -0.8111,
        -0.6718, -0.2098, -0.9037, -2.1639,  1.3600,  0.3710, -0.2914, -4.3348,
         0.6479, -0.4050, -0.4376, -0.5848, -1.7765, -0.1220, -1.6271, -0.0587,
        -0.0313,  1.1933, -0.8193, -0.5881, -1.3885, -0.0624, -0.0135, -0.0469,
        -0.1871, -0.1578, -0.0736, -0.9998, -0.1053, -1.3701, -0.8906, -0.0332,
        -0.6834, -2.0189, -0.6967, -0.0536, -0.0215, -1.6603, -2.1980,  3.4264,
        -3.0332, -0.5934, -0.7362, -2.1855,  0.9343, -1.7800, -1.3953, -2.4384,
        -0.5082, -2.0626, -1.8196,  1.5706, -0.4842, -0.0466,  0.3909,  0.9127,
        -0.5369,  5.2684, -0.4638, -0.4271, -0.1466, -0.4782, -1.0297, -1.1149,
        -0.5026, -0.6495,  6.9399, -1.7225, -0.0237, -0.2928, -0.1810, -0.4540,
         0.0151, -0.1378,  1.1891, -0.3362, -2.5073, -0.0589,  0.5431,  0.4689],
       requires_grad=True))

Name: layer4.scale
Value: 528.794677734375

Name: layer4.zero_point
Value: 145

Name: layer4._packed_params.dtype
Value: torch.qint8

Name: layer4._packed_params._packed_params
Value: (tensor([[-0.0549, -0.1464, -0.3111,  ...,  0.0000,  0.0000, -0.2745],
        [-0.5124,  0.0366, -0.0549,  ...,  0.0549,  0.0549, -0.0183],
        [-0.0549,  0.0183, -0.0915,  ...,  0.0000, -0.0549,  0.0000],
        ...,
        [-0.0549, -0.1830, -0.0549,  ...,  0.0366,  0.0366,  0.0915],
        [-0.2013,  0.0915, -0.0366,  ..., -0.3660, -0.0549, -0.0183],
        [ 0.0000,  0.0549, -0.0732,  ..., -0.0915,  0.0183, -0.0732]],
       size=(32, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.018301451578736305,
       zero_point=2), Parameter containing:
tensor([-0.1303, -0.4316, -0.0401, -0.2923, -0.1829, -0.4082, -0.0598,  1.4863,
        -0.1085,  0.0664, -0.3013, -0.1855, -1.8090, -0.0269, -0.1343,  0.0638,
        -0.2201, -0.1402, -0.5462, -0.1405, -1.3675,  0.0946,  0.0157, -0.0313,
        -0.0661, -0.0692, -0.0121,  0.0173, -0.0790,  0.4442,  0.1450, -0.0749],
       requires_grad=True))

Name: layer5.scale
Value: 199.5968475341797

Name: layer5.zero_point
Value: 1

Name: layer5._packed_params.dtype
Value: torch.qint8

Name: layer5._packed_params._packed_params
Value: (tensor([[ 0.1858, -0.1419, -0.0574,  0.3007,  0.0135,  0.0338,  0.2162,  0.0135,
          0.0811,  0.0507, -0.2703,  0.0169, -0.2500,  0.0980,  0.1318,  0.0574,
          0.1723,  0.0541,  0.0676, -0.0642, -0.1351,  0.0236,  0.0270,  0.1453,
          0.1047,  0.0608,  0.2128, -0.1047, -0.3818,  0.0845,  0.1014, -0.0405],
        [-0.0169, -0.2872,  0.0777,  0.4798,  0.3176,  0.0169,  0.2095,  0.0135,
          0.1014,  0.0439,  0.1960, -0.2331, -0.0338, -0.0034,  0.1622, -0.0405,
          0.0135,  0.1081,  0.0135,  0.0507, -0.0135,  0.0439,  0.0777,  0.1216,
          0.1791,  0.2264,  0.1655, -0.3074,  0.1993,  0.0845, -0.0203, -0.1250],
        [-0.0270, -0.3547, -0.0338, -0.2838,  0.1216,  0.2196, -0.0608,  0.1487,
         -0.0473,  0.1014,  0.2399, -0.2568,  0.3345, -0.0169,  0.0270,  0.1014,
          0.3919, -0.0608,  0.1757, -0.0608,  0.0203,  0.3379, -0.0709, -0.1115,
         -0.0372,  0.0372,  0.0270, -0.0743, -0.1487,  0.1858,  0.0034, -0.0574],
        [ 0.2027, -0.2838,  0.0439, -0.1791,  0.0101,  0.1115,  0.1723, -0.0034,
          0.2838,  0.0439,  0.2838, -0.1351, -0.0338,  0.0439,  0.1520,  0.0642,
          0.0743, -0.0743,  0.0203,  0.0169, -0.0608,  0.0034,  0.1554,  0.0101,
          0.1723, -0.1081,  0.0169, -0.0034, -0.0270,  0.0946, -0.0980,  0.1892]],
       size=(4, 32), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.003378547029569745,
       zero_point=-15), Parameter containing:
tensor([9.6215, 1.1825, 0.2378, 2.2440], requires_grad=True))

Name: quant.scale
Value: tensor([1.4972])

Name: quant.zero_point
Value: tensor([0])

