Name: layer1.scale
Value: 12.041574478149414

Name: layer1.zero_point
Value: 76

Name: layer1._packed_params.dtype
Value: torch.qint8

Name: layer1._packed_params._packed_params
Value: (tensor([[ 0.1870, -0.3740, -0.0935,  ..., -0.4674, -0.0935,  0.0000],
        [ 0.1870, -0.3740, -0.2805,  ...,  0.0000,  0.0935,  0.1870],
        [-1.8698,  0.1870, -0.3740,  ...,  0.0935, -0.0935,  0.0000],
        ...,
        [-0.1870, -0.6544, -0.1870,  ..., -1.4023, -0.6544, -0.0935],
        [ 0.0000, -0.2805, -0.0935,  ..., -0.1870, -0.2805,  0.1870],
        [-1.2154,  0.3740, -0.2805,  ...,  0.5609, -0.2805, -0.2805]],
       size=(1024, 7), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.09348944574594498,
       zero_point=78), Parameter containing:
tensor([ 0.1467, -0.2225, -1.5777,  ...,  0.1372,  0.0781, -0.8142],
       requires_grad=True))

Name: layer2.scale
Value: 302.292724609375

Name: layer2.zero_point
Value: 110

Name: layer2._packed_params.dtype
Value: torch.qint8

Name: layer2._packed_params._packed_params
Value: (tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.4467,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.2233]],
       size=(512, 1024), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.22333493828773499,
       zero_point=80), Parameter containing:
tensor([-0.0621, -0.0422, -0.0614, -0.0718, -0.0711, -0.1990, -1.8157, -2.2279,
        -0.0630, -0.0395, -0.1791, -0.0471, -0.0944, -0.2815, -0.1427, -0.1338,
        -2.0703, -0.0730, -0.0785, -0.0559, -2.2446, -1.8600, -0.0900, -0.0520,
        -0.0459, -0.0719, -0.2852, -2.1815, -2.4101, -1.6226, -2.5907, -2.1501,
        -2.3240, -1.7309, -0.0924, -2.3973, -2.2942, -0.0758, -3.0544, -0.0859,
        -2.2050, -0.0424, -0.0371, -2.3363, -0.0630, -0.0541, -2.3377, -2.4587,
        -0.3709, -0.2109, -2.2268, -0.2347, -0.0618, -0.2970, -0.0541, -0.0458,
        -0.2740, -0.0891, -0.0502, -1.3237, -0.0825, -2.0701, -0.0365, -2.3435,
        -0.0603, -0.0695, -2.3683, -0.0478, -2.4936, -2.3803, -0.3490, -0.0494,
        -0.0507, -0.0819, -0.0598, -0.0520, -1.6675, -2.2821, -0.0522, -2.4274,
        -0.0712, -0.0335, -0.0386, -2.0057, -2.4189, -2.3542, -0.0838, -0.0401,
        -2.4903, -0.0532, -1.8694, -1.8024, -0.0986, -0.0537, -2.2102, -0.0311,
        -2.4819, -2.4280, -2.2862, -0.0775, -0.1846, -0.0296, -0.2064, -2.4249,
        -0.0590, -1.9093, -2.0751, -0.1862, -0.1047, -2.1221, -1.4322, -0.0501,
        -2.2567, -0.1545, -0.2006, -0.0388, -0.0337, -0.0547, -2.6011, -1.8160,
        -0.0656, -0.0371, -2.5737, -0.0562, -2.2277, -2.4065, -0.0669, -2.2715,
        -0.0399, -2.2755, -0.3322, -0.0482, -0.0714, -2.2801, -2.2905, -1.9139,
        -0.1919, -0.0330, -1.9089, -0.2186, -0.0337, -2.1692, -2.5965, -2.4027,
        -0.0577, -0.0398, -0.0788, -0.0750, -2.6393, -1.8684, -1.4371, -2.5230,
        -0.2184, -2.3929, -0.0532, -3.0948, -0.0825, -2.3848, -0.0729, -2.4240,
        -2.5561, -2.3865, -0.0520,  0.3411, -0.0537, -0.0321, -2.4308, -2.5399,
        -0.0340, -2.0361, -0.0528, -2.0046, -0.4389, -0.0516, -2.5472, -0.0517,
        -0.0580, -0.0872, -0.0882, -2.3086, -1.7410, -0.0427, -1.0750, -1.2177,
        -0.1692, -0.0846, -0.0862, -2.1184, -0.0718, -1.8127, -0.0583, -0.0819,
        -0.0855, -0.0647, -0.0745, -0.0764, -0.0699, -0.2642, -2.0210, -2.5624,
        -0.0433, -2.3701, -0.1535, -0.0829, -1.5445, -2.4297, -1.8336, -0.0830,
        -0.0759, -0.0678, -0.0997, -2.4114, -2.1574, -2.1099, -0.1072, -0.0655,
        -0.0267, -2.9422, -2.4467, -0.1652, -0.0320, -0.1681, -0.0741, -0.0846,
        -0.0739, -2.5847, -2.5697, -2.0692, -0.0864, -0.0358, -2.2084, -0.0909,
        -2.2821, -0.0609, -2.3958, -0.0599, -1.0101, -0.7672, -2.8915, -1.8085,
        -2.2683, -2.2059, -2.0824, -0.0452, -0.0887, -0.4681, -0.0202, -0.0387,
        -2.4272, -2.3072,  0.1350, -0.0430, -0.0673, -2.2838, -2.3299, -2.5872,
        -1.5069,  0.0254, -0.0884, -0.0402, -1.5670, -0.0654, -0.2498, -2.2927,
        -2.3227, -1.7723, -0.1517, -2.0205, -2.5630, -0.2766, -2.0737, -2.1871,
        -2.3318, -0.0781, -2.2890, -2.4420, -0.0325, -0.0799, -0.0844, -0.0883,
        -0.0616, -0.0802, -0.0487, -2.3789, -2.2296, -0.0819, -0.0875, -0.0639,
        -2.2260, -0.0278, -2.1048, -0.4009, -1.1235, -0.0754, -2.8088, -0.0509,
        -0.0856, -0.0346, -1.9989, -0.0789, -0.0416, -2.4366, -0.0322, -2.0334,
        -0.0396, -0.0555, -2.7606, -0.2724, -0.0900, -3.8511, -1.5480, -0.0321,
        -2.1531, -0.0768,  0.9135, -0.1195, -2.5523, -0.0444, -2.2289, -0.2777,
        -0.5916, -0.1096, -2.0259, -2.0739, -0.0902, -0.0555, -2.3688, -0.0452,
        -0.1679, -1.4747, -1.7695, -0.0852, -0.0494, -2.0251, -0.0541, -0.0333,
        -0.0393, -0.0714, -0.0832, -1.6622, -1.8006, -1.9513, -0.3228, -0.0858,
        -0.0507, -2.2670, -2.0789, -0.1886, -2.1157, -2.3042, -2.0299, -0.0287,
        -0.0873, -0.0403, -0.0395, -0.0349, -1.9445, -0.0708, -1.7570, -2.4369,
        -1.6276, -0.0752, -0.1174, -0.2833, -2.3453, -0.1876,  3.7916, -0.0696,
        -0.0869, -0.1967, -0.0478, -1.9211, -2.4191, -2.3147, -0.0690, -0.0739,
        -2.1738, -0.0618, -0.0475, -1.5046, -0.0705, -0.1846, -2.3533, -2.3992,
        -0.1112, -0.0672, -0.0425, -2.1973, -2.4829, -0.0631, -3.1437, -2.2396,
        -2.3748,  0.2623, -0.0606, -0.0299, -0.0357, -0.0555, -0.3800, -1.9303,
        -0.0751, -0.0791, -0.0795, -2.1897, -0.0641, -0.0808, -0.2686, -1.9150,
        -0.0475, -1.6895, -0.0298, -0.7867, -0.1059, -0.0653, -0.0456, -0.0890,
        -0.0301, -0.0802, -2.2839, -2.3095, -0.2876, -0.0298, -0.0998, -2.5694,
        -2.2101, -2.2337, -0.0659, -2.3536, -2.1615, -0.0751, -2.3421, -1.6295,
        -2.1724, -0.0504, -2.1267, -0.0642, -0.0792, -2.3931, -0.0378, -2.3122,
        -2.0148, -0.0769, -2.5146, -0.0751, -0.0873, -0.0786,  0.0923, -0.7278,
        -0.0458, -1.4472, -0.3531, -2.2413, -0.0868, -0.0580, -0.0445, -0.2855,
        -0.0867, -0.3120, -1.9263, -0.0562, -0.0321, -0.3194, -1.8351, -0.0614,
        -1.8309, -2.1394, -1.8420, -2.6306, -0.0518, -0.0673, -2.1811, -0.0825,
        -0.0785, -2.3715, -2.3844, -0.0908, -2.2126, -0.0344, -0.0447, -2.3917,
        -0.0338, -1.5983, -0.0678, -2.6728, -0.0906, -2.3492, -2.1270, -0.0543,
        -0.0638, -0.1565, -0.0871, -0.0243, -2.2795, -0.0908, -0.1868, -0.0465,
        -3.9357, -2.2644, -0.1775, -0.0828, -2.2436, -2.4535, -0.0440, -0.0327,
         0.6093, -2.1469, -0.2972, -1.9511, -0.0361, -0.1147, -0.3472, -0.3693],
       requires_grad=True))

Name: layer3.scale
Value: 1202.860107421875

Name: layer3.zero_point
Value: 92

Name: layer3._packed_params.dtype
Value: torch.qint8

Name: layer3._packed_params._packed_params
Value: (tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0927, -0.2781,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ..., -0.0927,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.2781,  0.0000],
        ...,
        [-0.0927, -0.0927, -0.0927,  ..., -0.0927, -0.0927,  0.0000],
        [ 0.0927,  0.0000,  0.0927,  ..., -0.0927,  0.0927,  0.0000],
        [ 0.0000, -0.0927, -0.0927,  ...,  0.0000, -0.3708,  0.0927]],
       size=(128, 512), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.09270480275154114,
       zero_point=76), Parameter containing:
tensor([-3.4346, -0.2732, -2.4879, -0.3165, -0.2811, -0.3333, -4.3726, -0.3542,
        -1.3520, -1.9466, -0.8466, -0.0368, -0.0295, -0.2360,  0.4696, -1.1601,
        -0.6633, -1.6422, -0.0507, -0.5540, -1.1505, -0.9505, -1.3947,  1.6321,
         1.5045, -1.5781, -0.4563,  0.8511, -0.1677,  0.5118, -1.3081, -0.2401,
        -0.0435, 11.9924, -0.7662, -0.7010, -0.5037, -0.7858, -0.1092, -0.0362,
        -0.1010, -0.3812, -0.3521,  0.5357, -1.4428, -1.7054, -0.3309, -0.8111,
        -0.5713, -0.4793, -0.9902, -2.2491,  1.3782,  0.3710, -0.2914, -3.8345,
         0.7396, -0.4692, -0.3495, -0.5848, -1.7765, -0.1220, -1.4884, -0.0587,
        -0.0313,  1.1933, -0.6620, -0.5881, -1.3124, -0.0624,  0.1493, -0.0469,
        -0.1871, -0.2778, -0.0736, -0.9998, -0.1053, -1.3510, -0.8906, -0.0332,
        -0.6688, -2.1374, -0.6927, -0.0700, -0.0215, -1.9505, -2.0247,  3.6498,
        -3.2538, -0.5934, -0.7362, -2.1234,  1.2061, -1.7346, -1.3237, -2.3293,
        -0.5082, -1.9494, -1.6764,  1.5706, -0.4842, -0.5415,  0.4289,  0.7079,
        -0.5369,  5.3616, -0.4638, -0.4271, -0.1946, -0.5724, -1.0297, -1.0867,
        -0.5026, -0.6499,  7.1525, -1.7225, -0.0237, -0.2928, -0.1810, -0.4540,
        -0.1860, -0.0604,  0.8910, -0.4041, -2.4418, -0.0589,  0.5431,  0.4982],
       requires_grad=True))

Name: layer4.scale
Value: 1180.70947265625

Name: layer4.zero_point
Value: 72

Name: layer4._packed_params.dtype
Value: torch.qint8

Name: layer4._packed_params._packed_params
Value: (tensor([[-0.0739, -0.1479, -0.2958,  ...,  0.0000,  0.0000, -0.2588],
        [-0.5176,  0.0370, -0.0739,  ...,  0.0370,  0.0739,  0.0000],
        [-0.0370,  0.0370, -0.0739,  ...,  0.0000, -0.0370,  0.0000],
        ...,
        [-0.0739,  0.0000, -0.0739,  ...,  0.0370,  0.0370,  0.1479],
        [-0.1849,  0.1109, -0.0370,  ..., -0.3697, -0.0370, -0.0370],
        [ 0.0000,  0.0370, -0.0739,  ..., -0.0739,  0.0370, -0.0739]],
       size=(32, 128), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.03697272390127182,
       zero_point=64), Parameter containing:
tensor([-0.1303, -0.4316, -0.0401, -0.2923, -0.1829, -0.3334, -0.0598,  1.1295,
        -0.1085, -0.0032, -0.3013, -0.1855, -1.7147, -0.0269, -0.1343,  0.1182,
        -0.1411, -0.1402, -0.4553, -0.1405, -1.3572, -0.3620,  0.0157, -0.0313,
        -0.0661, -0.0692, -0.0121,  0.0173, -0.0790,  0.0755,  0.1450, -0.0749],
       requires_grad=True))

Name: layer5.scale
Value: 458.76153564453125

Name: layer5.zero_point
Value: 0

Name: layer5._packed_params.dtype
Value: torch.qint8

Name: layer5._packed_params._packed_params
Value: (tensor([[ 0.1832, -0.1425, -0.0543,  0.2985,  0.0136,  0.0271,  0.2171,  0.0000,
          0.0814, -0.0814, -0.2713,  0.0204, -0.3188,  0.0950,  0.1289,  0.1628,
          0.2103,  0.0543,  0.1153, -0.0611, -0.0407, -0.0814,  0.0271,  0.1425,
          0.1018,  0.0611,  0.2171, -0.1018, -0.3799,  0.0882,  0.1018, -0.0407],
        [-0.0136, -0.2849,  0.0814,  0.4816,  0.3188,  0.0136,  0.2103,  0.0000,
          0.1018,  0.0000,  0.1967, -0.2306, -0.0543,  0.0000,  0.1628,  0.0339,
          0.0339,  0.1085,  0.0136,  0.0543, -0.0204, -0.0204,  0.0814,  0.1221,
          0.1832,  0.2306,  0.1628, -0.3053,  0.1967, -0.0068, -0.0204, -0.1221],
        [-0.0271, -0.3528, -0.0339, -0.2849,  0.1221,  0.2239, -0.0611,  0.2239,
         -0.0475,  0.0407,  0.2374, -0.2578,  0.3392, -0.0136,  0.0271, -0.0407,
          0.3935, -0.0611,  0.1832, -0.0611,  0.0407,  0.0543, -0.0746, -0.1085,
         -0.0407,  0.0407,  0.0271, -0.0746, -0.1492,  0.0271,  0.0000, -0.0543],
        [ 0.2035, -0.2849,  0.0475, -0.1764,  0.0136,  0.1357,  0.1764,  0.0882,
          0.2849, -0.0271,  0.2849, -0.1357, -0.0475,  0.0407,  0.1492,  0.0000,
          0.0678, -0.0746,  0.0271,  0.0136, -0.1764, -0.1899,  0.1560,  0.0136,
          0.1696, -0.1085,  0.0204, -0.0068, -0.0271, -0.0678, -0.1018,  0.1899]],
       size=(4, 32), dtype=torch.qint8,
       quantization_scheme=torch.per_tensor_affine, scale=0.006783696822822094,
       zero_point=56), Parameter containing:
tensor([9.3459, 0.1947, 0.1517, 2.3623], requires_grad=True))

Name: quant.scale
Value: tensor([3.0079])

Name: quant.zero_point
Value: tensor([0])

